[Inception]

[EfficientNet]
version = B0
model_dir = C:\viNet_RnD\Training\EfficientNet_saves\viNet_2.8_Goldwind_3_Class_Clean
; pick up from where an old train ended
checkpoint_dir = None
; log_dir for tensorboard
log_dir = C:\viNet_RnD\Log\Vattenfall-Gotland\viNet_2.8_Goldwind_3_Class_Clean
num_classes = 3
; I only really tested adam
optimizer = adam
momentum = 0.9
learning_rate = 0.0001
loss = sparse_categorical_cross_entropy
; metrics are comma separated (and loss is implied)
metrics = accuracy
evaluate_metrics = accuracy,categorical_accuracy

; default is to use only one GPU or only CPU if there are no GPUs
; to use multiple GPUs you need to use a mirror strategy, i like `mirror_strategy_hierarchical_copy`
distributed_strategy = default

batch_size = 8

log_every_n_steps: 100

max_steps_per_epoch = None
num_validation_images = 50
epochs = 40

bench_timings = False
bench_file = C:\viNet_RnD\Bench\data.csv

[Dataset]
train_dataset_dir = C:\viNet_RnD\Datasets\Goldwind-CattleHill\TFRecords\viNet_2.8_Goldwind_3_Class_Clean\train
; if validation_dataset_dir is none then train_dataset_dir will be given when asking for the validation_dataset_dir
validation_dataset_dir = C:\viNet_RnD\Datasets\Goldwind-CattleHill\TFRecords\viNet_2.8_Goldwind_3_Class_Clean\validation
